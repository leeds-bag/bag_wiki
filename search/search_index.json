{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the BAG Wiki! This is a wiki for Leeds Biosphere and Atmosphere Group (BAG) to share computing and technical guides and tips Contributing If you'd like to contribute, thank you! There a few ways to do this: * With a GitHub account invited to this Organisation, you can add files (tutorials etc) and publish website changes * To add files to the website, go to the docs folder and create or upload a file * Commit this change using the button, writing a short description of your change * Link your file on the website, by editing 'mkdocs.yml' and listing your file information in the how to guides section * Without permissions, you can submit text files, word docs, markdown files to admin and they can be uploaded on your behalf","title":"Home"},{"location":"#welcome-to-the-bag-wiki","text":"This is a wiki for Leeds Biosphere and Atmosphere Group (BAG) to share computing and technical guides and tips","title":"Welcome to the BAG Wiki!"},{"location":"#contributing","text":"If you'd like to contribute, thank you! There a few ways to do this: * With a GitHub account invited to this Organisation, you can add files (tutorials etc) and publish website changes * To add files to the website, go to the docs folder and create or upload a file * Commit this change using the button, writing a short description of your change * Link your file on the website, by editing 'mkdocs.yml' and listing your file information in the how to guides section * Without permissions, you can submit text files, word docs, markdown files to admin and they can be uploaded on your behalf","title":"Contributing"},{"location":"VSCode_guide/","text":"Guide to VS Code by Ben Bradley, 29/03/2025 (last updated 04/11/2025) \ud83c\udfc1 Introduction This document is intended as a guide to modern programming for researchers and students within the School of Earth and Environment, primarily using Python and VS Code. Throughout, I draw upon the expertise shared in the following sources: - The BAG Wiki , particularly the Guide to VS Code - The Good Research Code Handbook Contents \u2753 What is VS Code and Python? \ud83c\udfd7\ufe0f How do I set this up? \u2753 VS Code and Python Python Python is one of the most successful programming languages, and for a reason! It values readability above all else, making it very user-friendly. It is open-source and primarily relies on importing libraries others have created to solve problems efficiently. This means you can lean on the work of others, who have likely solved the problem far more efficiently and cleanly than you could! Many people start using Python via the Python-provided Integrated Development Environment (IDE) or directly through the terminal. These are excellent places to get to grips with Python when first learning the language, as they allow you to focus on the basic coding you'll encounter in tutorials. Complications arise when you want to do more than the basics, though. Environment Managers Python relies on libraries for most interesting things anyone might actually want to do. Perhaps you've found yourself in the following situation before? discover a cool thing you want to try out in Python find you need a specific library import library doesn't work! google the error message find that you need to install the library using a command called pip your computer doesn't recognise the command pip other people say to use conda install but this doesn't work either some random guy from StackOverflow says everything will work if you use pip3 give up You need a package manager ! Setting this up means you can easily and dependably install any libraries you may need. Once you start installing lots of libraries, you'll eventually find that some conflict with others. If you get unlucky, you'll install a library and find that your code no longer runs! To avoid this, you need a virtual environment manager : something that resolves package dependencies and installs libraries in virtual environments that can easily be deleted and refreshed if something goes wrong. Some popular examples of joint package and virtual environment managers are Anaconda , Miniconda , and Miniforge , which we'll look at later. Without one of these, your Python environment can become a messy hellscape :( VS Code But if you're doing lots of coding, you don't just want your code to run . Is it clear, efficient, well-documented, backed-up, shareable? This is where more complete IDEs like VS Code come in! Visual Studio Code (VS Code) offers many attractive features, such as: - Multi-language support - Variable auto-completion syntax highlighting - Robust terminal interfaces AND intuitive feature-rich graphical interfaces - Seamless version control with Git and GitHub - Jupyter Notebooks! - Easy ways to explore netcdfs, tables, plots, and files - Installable extensions like auto-generated documentation, AI coding suggestions, and more! Hopefully this convinces you that VS Code is worth the effort to set up and use for day-to-day coding tasks. We'll explore how to set this up in the following section. (For more information, visit the BAG wiki guide to VS Code ) \u26a0\ufe0f IMPORTANT: A Note on VS Code Compatibility At the moment, the university is transitioning its \"general compute\" resource for the faculty of environment from the old foe-linux machines that run on CentOS to modern machines run with RedHat9. The old machines have outdated software, requiring a downgrade to version 1.98 of VS Code to enable use. The new machines are, as of 06/10/2025, not fully configured and still have some problems. New and existing users therefore have a choice of two paths, both of which are described in this guide: 1) Downgrade VS Code and make use of the more reliable old machines, with some potential hiccups from VS Code and its extensions to navigate 2) Install the latest version of VS Code and be prepared to report some problems as the system is set up (contact Steve Arnold for this). It is currently recommended to choose option 1, but all researchers will eventually need to migrate to the new system. \ud83c\udfd7\ufe0f Setting Up 1) University VPN At the time of writing there are two types of university VPN: PulseSecure and Ivanti Secure Access. One of these is needed to connect to university systems remotely. Already installed If you have a university computer, the VPN may be pre-installed. For Windows, check by going to the start menu and typing \u201cPulse Secure\u201d or \"Ivanti Secure Access\". Uni laptop but not pre-installed If PulseSecure isn\u2019t already available, it can be installed via Company Portal. Search the start menu for \u201cCompany Portal\u201d. PulseSecure should be listed under the 'Apps' section. Personal computer Go to the University of Leeds VPN guide . Under attachments to the right of the screen, there is a list of VPN versions you can download. Select the appropriate download for your operating system and follow the setup wizard instructions. ![png](assets/PulseSecure_download.png) Once you have the VPN installed, follow the instructions on the VPN guide , which detail how to connect. (n.b. an alternative method to this is to use the rash jump server: I have found this way requires more passwords and a Duo sign-in, but university IT plans to switch to this method at some point. A guide for using rash can be found here: BAG guide to rash ) 2) Installing VS Code Install VS Code It is recommended to download VSCode version 1.98 on a local PC if you plan to use the old foe-linux machines. (If using the new foe-linux-cpu machines, simply download the latest version here and ignore the rest of this step). When you download an outdated version of VSC (e.g. February 2025, version 1.98) make sure you are downloading the version from the third line in the page, not from the more interesting blue button (this is the latest version, the name is in the downloaded file anyway). Additionally, to avoid VS Code auto-updating the next day, follow the next steps: Open Settings by pressing Ctrl + , Type \"update mode\" in the search bar Find the setting labelled \"Update: Mode\" and change to \"None\" to disable auto-updates Click the \"Restart\" button to apply the changes. Extensions Open VS Code and click the extensions tab on the left side of the screen. Search for and install the Python , Jupyter , and Remote - SSH extensions. These are needed to use Jupyter notebooks and access remote machines. If you downloaded a downgraded version of VS Code in the previous step, you'll also need to downgrade these extensions: Jupyter (install specific version: 2024.11.0 & turn off auto-update) Python (install specific version: 2025.12.0 & turn off auto-update) Pylance (install specific version: 2025.4.1 & turn off auto-update) Remote SSH (no downgrade necessary, as far as we know!) 3) config file To ssh easily into remote machines like foe-linux, it's easiest to create a config file containing the details of the ssh connections you want to make. This is a file called config stored in the .ssh folder of your local machine. For linux users, the full path location will simply be ~/.ssh/config . For Windows users, this might be in a different location but you'll likely have a .ssh folder somewhere. For instance, the path to mine is C:\\Users\\{my username}\\.ssh\\config . Whether you can find it in your file explorer or not, you'll be able to edit your config file in VS Code. Detailed instructions are on the BAG Wiki if you get stuck, but in short: open the command pallete using Ctrl + Shift + P . search for \"Remote - SSH\". An option to open a config file should appear. Specify connections to at least a foe-linux server and additionally to a machine on foe-linux (silloth, lytham, uptonpark, etc.) by copying the text below. Save the config file, close VS Code and reopen. Host foe HostName foe-linux-01.leeds.ac.uk # change to a different foe-linux server if you want (e.g. foe-linux-02) ForwardX11Trusted yes ForwardAgent yes User (username) # change to your username TCPKeepAlive no RequestTTY force Host (machine_name) # change to the machine you want to connect to HostName (machine_name).leeds.ac.uk # change to the machine you want to connect to ForwardX11Trusted yes User (username) # change to your username ForwardAgent yes TCPKeepAlive no ProxyJump foe RequestTTY force ################################################### # some other useful hosts ################################################### Host rash # this is a jump server used as an alternative to a VPN connection, requires Duo authentication User (username) HostName rash.leeds.ac.uk ServerAliveInterval 60 ServerAliveCountMax 90 ForwardX11 yes ForwardX11Trusted yes ForwardAgent yes ControlPath ~/.ssh/sockets/%r@%h:%p ControlMaster auto ControlPersist 12h Host JASMIN-sci # requires setting up ssh keys Hostname sci-vm-04.jasmin.ac.uk User (username) ProxyJump (username)@login-01.jasmin.ac.uk ForwardAgent yes Host JASMIN-ph # requires setting up ssh keys Hostname sci-ph-02.jasmin.ac.uk User (username) ProxyJump (username)@login-01.jasmin.ac.uk ForwardAgent yes Host foe-cpu User (username) HostName foe-linux-cpu.leeds.ac.uk ServerAliveInterval 60 ServerAliveCountMax 90 ForwardX11 yes ForwardX11Trusted yes ForwardAgent yes ProxyJump rash.leeds.ac.uk Notes: - Remember to change the username with your own and the remote machine with one you want to access! - See the BAG Guides to SSH config files , SSH Keys , and VS Code for more details. - If you find VS Code is timing out before connecting, you can change the \"Connect Timeout\" value in \"Remote - SSH: Settings\". The BAG guide to VS Code tips section says setting this parameter to around 1080 should solve this (it did for me). - If you decide to use rash instead of the University VPN, you will need to add it as a host to your config file and ProxyJump through rash to connect to foe (see BAG wiki article ). - You could try to combine ssh keys with this sign in method to reduce the number of passwords required, but as of writing I have not attempted this. 4) ssh into remote machine ssh-ing Go to the Remote Explorer side pannel on the left. The host names you added to your config file should be listed under SSH. Connect to one of them. Depending on the connection, you may be prompted to enter your password multiple times. You may also be required to select the operating system of the machine you are connecting to (linux) and dismiss some popups. Connected! You should land in the remote machine with a page that looks something like the one below. Note the box in the bottom left which should display the name of the host you are connected to.![png](assets/ssh_welcome.png) Install Extensions Install the same extensions you installed in step 2b on the remote machine, again by navigating to the extensions side tab. Open project directory From here you can open the directory for your coding project, allowing you to view and navigate all files in your project! Open the Explorer side pannel on the left and click Open Folder. You can then navigate to a project directory of your choice. You will need to inseret your password again as the connection reestablishes from the new directory. If you don't already have a folder, you can create one in the terminal (accessed via VS Code through View > Terminal or Terminal > New Terminal). Closing the connection Once you're finished, remember to save all documents and close the remote connection. To do this, click the SHH block in the bottom left, then choose \"Close Remote Connection\". Shortcut for next time The next time you connect, you can connect directly to the folder you opened on the remote machine, without having to open the project folder and re-input your passwords! In the image below, VS Code has remembered me connecting to my project_wetland folder via uptonpark, and I can click either one to connect directly to that folder. 5) Set up an environment manager Now you need to install an environment manager on the machine you want to code on. You use this to create virtual environments and install libraries. If you encounter dependency problems within your environment, you can easily create a new one and install a new set of libraries there. You need to set this up wherever you do your coding, so ideally on a disk in foe-linux ( NOT your home directory as you won't have enough space). It's good practice to create a new environment for each project. There are a couple of options to choose from. I have found that Miniforge works best for Linux machines, as it installs packages via conda-forge by default, making it faster and more reliable than other methods. However, I have had problems installing it locally on my university Windows laptop as it gets flagged as malware by the IT security. Miniconda is a good alternative for local work on university Windows computers. (\u26a0\ufe0fNote that taught students will need to change their .bashrc file to allow installation of Miniforge, see details below) The following instructions are for installing Miniforge on a disk connected to foe-linux. Disk access You will need to be allocated a username directory on one of the disks connected to foe-linux. Once you have this set up, use VS Code to SSH into foe-linux or a machine connected to foe, as described in the previous section. Open a terminal (Terminal > New Terminal) and navigate to your username directory on one of the disks where you want Miniforge installed (e.g. cd /nfs/b0249/Users/$USER/ ). Installing Miniforge The full instructions are available in the README document on the Miniforge repository (or the Miniconda webpage if you prefer to install this). For Miniforge, these are under the \"Unix-like platforms (macOS, Linux, & WSL)\" header under \"Install\". I'll summarise the commands here, but make sure you run these on a disk, not your home directory! Download the installer using wget \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\" Now run the intaller using bash Miniforge3-$(uname)-$(uname -m).sh When asked where to install Python, don't accept the default suggestion, type in the filepath of the disk (i.e. /nfs/b0249/Users/$USER/minforge3) to install it there. Create a test environment Once you've completed the install instructions, be sure to activate the base environment by typing conda activate in the terminal. The terminal should now show (base) at the start of the command line. If you type conda list , a list of the basic packages installed in this environment will be listed. Now type conda create -n test_env to create a new environment called \"test_env\". You need to activate this new environment by typing conda activate test_env . You can deactivate an environment by typing conda deactivate . Useful Python libraries Create a new environment (rename it something memorable) and install some useful Python libraries: matplotlib and cartopy for basic plotting xarray, netcdf4, and dask for handling gridded datasets rioxarray and geopandas for shapefiles xesmf for regridding notebook for all Jupyter notebook-related libraries conda create -n myenv conda activate myenv conda install matplotlib cartopy xarray netcdf4 dask rioxarray geopandas xesmf notebook Jupyter Notebook Create a new Jupyter notebook test_notebook.ipynb by clicking \"New File\" under the Explorer side panel and naming it. Create a code cell and add some simple code. In the top right of the screen, click the \"Select Kernel\" button. Under \"Python Environments\" you should find the environment you created earlier with notebook installed. Once this is selected, run the code cell: it should run. \u26a0\ufe0fFor taught students (e.g. Master's or Undergraduate students): Before attempting the steps above for setting up the environment manager Miniforge, you will need to modify the .bashrc file in the home directory of your foe-linux account to be able to do this. Open this file by typing nano ~/.bashrc in a terminal connected to foe-linux. Student .bashrc files may have a line source /nfs/see-fs-01_teaching/.bashrc which should be commented out by prepending a # . Now save and close the file, logout and back in again and follow the steps starting from 5a.","title":"Using VSCode"},{"location":"VSCode_guide/#guide-to-vs-code","text":"by Ben Bradley, 29/03/2025 (last updated 04/11/2025)","title":"Guide to VS Code"},{"location":"VSCode_guide/#introduction","text":"This document is intended as a guide to modern programming for researchers and students within the School of Earth and Environment, primarily using Python and VS Code. Throughout, I draw upon the expertise shared in the following sources: - The BAG Wiki , particularly the Guide to VS Code - The Good Research Code Handbook","title":"\ud83c\udfc1 Introduction"},{"location":"VSCode_guide/#contents","text":"\u2753 What is VS Code and Python? \ud83c\udfd7\ufe0f How do I set this up?","title":"Contents"},{"location":"VSCode_guide/#vs-code-and-python","text":"","title":"\u2753 VS Code and Python"},{"location":"VSCode_guide/#python","text":"Python is one of the most successful programming languages, and for a reason! It values readability above all else, making it very user-friendly. It is open-source and primarily relies on importing libraries others have created to solve problems efficiently. This means you can lean on the work of others, who have likely solved the problem far more efficiently and cleanly than you could! Many people start using Python via the Python-provided Integrated Development Environment (IDE) or directly through the terminal. These are excellent places to get to grips with Python when first learning the language, as they allow you to focus on the basic coding you'll encounter in tutorials. Complications arise when you want to do more than the basics, though.","title":"Python"},{"location":"VSCode_guide/#environment-managers","text":"Python relies on libraries for most interesting things anyone might actually want to do. Perhaps you've found yourself in the following situation before? discover a cool thing you want to try out in Python find you need a specific library import library doesn't work! google the error message find that you need to install the library using a command called pip your computer doesn't recognise the command pip other people say to use conda install but this doesn't work either some random guy from StackOverflow says everything will work if you use pip3 give up You need a package manager ! Setting this up means you can easily and dependably install any libraries you may need. Once you start installing lots of libraries, you'll eventually find that some conflict with others. If you get unlucky, you'll install a library and find that your code no longer runs! To avoid this, you need a virtual environment manager : something that resolves package dependencies and installs libraries in virtual environments that can easily be deleted and refreshed if something goes wrong. Some popular examples of joint package and virtual environment managers are Anaconda , Miniconda , and Miniforge , which we'll look at later. Without one of these, your Python environment can become a messy hellscape :(","title":"Environment Managers"},{"location":"VSCode_guide/#vs-code","text":"But if you're doing lots of coding, you don't just want your code to run . Is it clear, efficient, well-documented, backed-up, shareable? This is where more complete IDEs like VS Code come in! Visual Studio Code (VS Code) offers many attractive features, such as: - Multi-language support - Variable auto-completion syntax highlighting - Robust terminal interfaces AND intuitive feature-rich graphical interfaces - Seamless version control with Git and GitHub - Jupyter Notebooks! - Easy ways to explore netcdfs, tables, plots, and files - Installable extensions like auto-generated documentation, AI coding suggestions, and more! Hopefully this convinces you that VS Code is worth the effort to set up and use for day-to-day coding tasks. We'll explore how to set this up in the following section. (For more information, visit the BAG wiki guide to VS Code )","title":"VS Code"},{"location":"VSCode_guide/#important-a-note-on-vs-code-compatibility","text":"At the moment, the university is transitioning its \"general compute\" resource for the faculty of environment from the old foe-linux machines that run on CentOS to modern machines run with RedHat9. The old machines have outdated software, requiring a downgrade to version 1.98 of VS Code to enable use. The new machines are, as of 06/10/2025, not fully configured and still have some problems. New and existing users therefore have a choice of two paths, both of which are described in this guide: 1) Downgrade VS Code and make use of the more reliable old machines, with some potential hiccups from VS Code and its extensions to navigate 2) Install the latest version of VS Code and be prepared to report some problems as the system is set up (contact Steve Arnold for this). It is currently recommended to choose option 1, but all researchers will eventually need to migrate to the new system.","title":"\u26a0\ufe0f IMPORTANT: A Note on VS Code Compatibility"},{"location":"VSCode_guide/#setting-up","text":"","title":"\ud83c\udfd7\ufe0f Setting Up"},{"location":"VSCode_guide/#1-university-vpn","text":"At the time of writing there are two types of university VPN: PulseSecure and Ivanti Secure Access. One of these is needed to connect to university systems remotely. Already installed If you have a university computer, the VPN may be pre-installed. For Windows, check by going to the start menu and typing \u201cPulse Secure\u201d or \"Ivanti Secure Access\". Uni laptop but not pre-installed If PulseSecure isn\u2019t already available, it can be installed via Company Portal. Search the start menu for \u201cCompany Portal\u201d. PulseSecure should be listed under the 'Apps' section. Personal computer Go to the University of Leeds VPN guide . Under attachments to the right of the screen, there is a list of VPN versions you can download. Select the appropriate download for your operating system and follow the setup wizard instructions. ![png](assets/PulseSecure_download.png) Once you have the VPN installed, follow the instructions on the VPN guide , which detail how to connect. (n.b. an alternative method to this is to use the rash jump server: I have found this way requires more passwords and a Duo sign-in, but university IT plans to switch to this method at some point. A guide for using rash can be found here: BAG guide to rash )","title":"1) University VPN"},{"location":"VSCode_guide/#2-installing-vs-code","text":"Install VS Code It is recommended to download VSCode version 1.98 on a local PC if you plan to use the old foe-linux machines. (If using the new foe-linux-cpu machines, simply download the latest version here and ignore the rest of this step). When you download an outdated version of VSC (e.g. February 2025, version 1.98) make sure you are downloading the version from the third line in the page, not from the more interesting blue button (this is the latest version, the name is in the downloaded file anyway). Additionally, to avoid VS Code auto-updating the next day, follow the next steps: Open Settings by pressing Ctrl + , Type \"update mode\" in the search bar Find the setting labelled \"Update: Mode\" and change to \"None\" to disable auto-updates Click the \"Restart\" button to apply the changes. Extensions Open VS Code and click the extensions tab on the left side of the screen. Search for and install the Python , Jupyter , and Remote - SSH extensions. These are needed to use Jupyter notebooks and access remote machines. If you downloaded a downgraded version of VS Code in the previous step, you'll also need to downgrade these extensions: Jupyter (install specific version: 2024.11.0 & turn off auto-update) Python (install specific version: 2025.12.0 & turn off auto-update) Pylance (install specific version: 2025.4.1 & turn off auto-update) Remote SSH (no downgrade necessary, as far as we know!)","title":"2) Installing VS Code"},{"location":"VSCode_guide/#3-config-file","text":"To ssh easily into remote machines like foe-linux, it's easiest to create a config file containing the details of the ssh connections you want to make. This is a file called config stored in the .ssh folder of your local machine. For linux users, the full path location will simply be ~/.ssh/config . For Windows users, this might be in a different location but you'll likely have a .ssh folder somewhere. For instance, the path to mine is C:\\Users\\{my username}\\.ssh\\config . Whether you can find it in your file explorer or not, you'll be able to edit your config file in VS Code. Detailed instructions are on the BAG Wiki if you get stuck, but in short: open the command pallete using Ctrl + Shift + P . search for \"Remote - SSH\". An option to open a config file should appear. Specify connections to at least a foe-linux server and additionally to a machine on foe-linux (silloth, lytham, uptonpark, etc.) by copying the text below. Save the config file, close VS Code and reopen. Host foe HostName foe-linux-01.leeds.ac.uk # change to a different foe-linux server if you want (e.g. foe-linux-02) ForwardX11Trusted yes ForwardAgent yes User (username) # change to your username TCPKeepAlive no RequestTTY force Host (machine_name) # change to the machine you want to connect to HostName (machine_name).leeds.ac.uk # change to the machine you want to connect to ForwardX11Trusted yes User (username) # change to your username ForwardAgent yes TCPKeepAlive no ProxyJump foe RequestTTY force ################################################### # some other useful hosts ################################################### Host rash # this is a jump server used as an alternative to a VPN connection, requires Duo authentication User (username) HostName rash.leeds.ac.uk ServerAliveInterval 60 ServerAliveCountMax 90 ForwardX11 yes ForwardX11Trusted yes ForwardAgent yes ControlPath ~/.ssh/sockets/%r@%h:%p ControlMaster auto ControlPersist 12h Host JASMIN-sci # requires setting up ssh keys Hostname sci-vm-04.jasmin.ac.uk User (username) ProxyJump (username)@login-01.jasmin.ac.uk ForwardAgent yes Host JASMIN-ph # requires setting up ssh keys Hostname sci-ph-02.jasmin.ac.uk User (username) ProxyJump (username)@login-01.jasmin.ac.uk ForwardAgent yes Host foe-cpu User (username) HostName foe-linux-cpu.leeds.ac.uk ServerAliveInterval 60 ServerAliveCountMax 90 ForwardX11 yes ForwardX11Trusted yes ForwardAgent yes ProxyJump rash.leeds.ac.uk Notes: - Remember to change the username with your own and the remote machine with one you want to access! - See the BAG Guides to SSH config files , SSH Keys , and VS Code for more details. - If you find VS Code is timing out before connecting, you can change the \"Connect Timeout\" value in \"Remote - SSH: Settings\". The BAG guide to VS Code tips section says setting this parameter to around 1080 should solve this (it did for me). - If you decide to use rash instead of the University VPN, you will need to add it as a host to your config file and ProxyJump through rash to connect to foe (see BAG wiki article ). - You could try to combine ssh keys with this sign in method to reduce the number of passwords required, but as of writing I have not attempted this.","title":"3) config file"},{"location":"VSCode_guide/#4-ssh-into-remote-machine","text":"ssh-ing Go to the Remote Explorer side pannel on the left. The host names you added to your config file should be listed under SSH. Connect to one of them. Depending on the connection, you may be prompted to enter your password multiple times. You may also be required to select the operating system of the machine you are connecting to (linux) and dismiss some popups. Connected! You should land in the remote machine with a page that looks something like the one below. Note the box in the bottom left which should display the name of the host you are connected to.![png](assets/ssh_welcome.png) Install Extensions Install the same extensions you installed in step 2b on the remote machine, again by navigating to the extensions side tab. Open project directory From here you can open the directory for your coding project, allowing you to view and navigate all files in your project! Open the Explorer side pannel on the left and click Open Folder. You can then navigate to a project directory of your choice. You will need to inseret your password again as the connection reestablishes from the new directory. If you don't already have a folder, you can create one in the terminal (accessed via VS Code through View > Terminal or Terminal > New Terminal). Closing the connection Once you're finished, remember to save all documents and close the remote connection. To do this, click the SHH block in the bottom left, then choose \"Close Remote Connection\". Shortcut for next time The next time you connect, you can connect directly to the folder you opened on the remote machine, without having to open the project folder and re-input your passwords! In the image below, VS Code has remembered me connecting to my project_wetland folder via uptonpark, and I can click either one to connect directly to that folder.","title":"4) ssh into remote machine"},{"location":"VSCode_guide/#5-set-up-an-environment-manager","text":"Now you need to install an environment manager on the machine you want to code on. You use this to create virtual environments and install libraries. If you encounter dependency problems within your environment, you can easily create a new one and install a new set of libraries there. You need to set this up wherever you do your coding, so ideally on a disk in foe-linux ( NOT your home directory as you won't have enough space). It's good practice to create a new environment for each project. There are a couple of options to choose from. I have found that Miniforge works best for Linux machines, as it installs packages via conda-forge by default, making it faster and more reliable than other methods. However, I have had problems installing it locally on my university Windows laptop as it gets flagged as malware by the IT security. Miniconda is a good alternative for local work on university Windows computers. (\u26a0\ufe0fNote that taught students will need to change their .bashrc file to allow installation of Miniforge, see details below) The following instructions are for installing Miniforge on a disk connected to foe-linux. Disk access You will need to be allocated a username directory on one of the disks connected to foe-linux. Once you have this set up, use VS Code to SSH into foe-linux or a machine connected to foe, as described in the previous section. Open a terminal (Terminal > New Terminal) and navigate to your username directory on one of the disks where you want Miniforge installed (e.g. cd /nfs/b0249/Users/$USER/ ). Installing Miniforge The full instructions are available in the README document on the Miniforge repository (or the Miniconda webpage if you prefer to install this). For Miniforge, these are under the \"Unix-like platforms (macOS, Linux, & WSL)\" header under \"Install\". I'll summarise the commands here, but make sure you run these on a disk, not your home directory! Download the installer using wget \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\" Now run the intaller using bash Miniforge3-$(uname)-$(uname -m).sh When asked where to install Python, don't accept the default suggestion, type in the filepath of the disk (i.e. /nfs/b0249/Users/$USER/minforge3) to install it there. Create a test environment Once you've completed the install instructions, be sure to activate the base environment by typing conda activate in the terminal. The terminal should now show (base) at the start of the command line. If you type conda list , a list of the basic packages installed in this environment will be listed. Now type conda create -n test_env to create a new environment called \"test_env\". You need to activate this new environment by typing conda activate test_env . You can deactivate an environment by typing conda deactivate . Useful Python libraries Create a new environment (rename it something memorable) and install some useful Python libraries: matplotlib and cartopy for basic plotting xarray, netcdf4, and dask for handling gridded datasets rioxarray and geopandas for shapefiles xesmf for regridding notebook for all Jupyter notebook-related libraries conda create -n myenv conda activate myenv conda install matplotlib cartopy xarray netcdf4 dask rioxarray geopandas xesmf notebook Jupyter Notebook Create a new Jupyter notebook test_notebook.ipynb by clicking \"New File\" under the Explorer side panel and naming it. Create a code cell and add some simple code. In the top right of the screen, click the \"Select Kernel\" button. Under \"Python Environments\" you should find the environment you created earlier with notebook installed. Once this is selected, run the code cell: it should run.","title":"5) Set up an environment manager"},{"location":"VSCode_guide/#for-taught-students-eg-masters-or-undergraduate-students","text":"Before attempting the steps above for setting up the environment manager Miniforge, you will need to modify the .bashrc file in the home directory of your foe-linux account to be able to do this. Open this file by typing nano ~/.bashrc in a terminal connected to foe-linux. Student .bashrc files may have a line source /nfs/see-fs-01_teaching/.bashrc which should be commented out by prepending a # . Now save and close the file, logout and back in again and follow the steps starting from 5a.","title":"\u26a0\ufe0fFor taught students (e.g. Master's or Undergraduate students):"},{"location":"conda_setup/","text":"Guide to Setting Up Mamba (Conda) on a Linux Machine Author: Callum What is Mamba? Mamba is a fast, robust, and user-friendly package manager for managing environments and packages in the Conda ecosystem. It is a drop-in replacement for the Conda command-line tool, written in C++ for speed and efficiency. Mamba uses the same environment and package specifications as Conda, but resolves dependencies and installs packages much faster, making it ideal for scientific computing, data science, and reproducible research. Why Use Mamba? Speed : Mamba is significantly faster than Conda, especially for solving complex dependencies and installing large packages. Reliability : Mamba uses parallel downloading and a more efficient dependency resolver, reducing installation errors and timeouts. Compatibility : Mamba works seamlessly with existing Conda environments and packages. You can use mamba and conda commands interchangeably. Community Support : Mamba is developed and maintained by the open-source community, with strong support from the conda-forge project. Installing Mamba via Miniforge Miniforge is a minimal installer for Conda and Mamba, maintained by conda-forge. It is recommended for Unix-like platforms (Linux, macOS, WSL) and comes with Mamba pre-installed. Step 1: Download the Miniforge Installer Open a terminal and download the installer appropriate for your system architecture. You can use wget : Using wget: wget \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\" Step 2: Run the Installer Script Run the downloaded script to install Miniforge (and Mamba): bash Miniforge3-$(uname)-$(uname -m).sh Follow the prompts to complete the installation. By default, Miniforge will install to your home directory (e.g., ~/miniforge3 ). Step 3: Initialize Conda After installation, initialize Conda in your shell: source ~/miniforge3/bin/activate conda init Restart your terminal to activate Conda automatically. Using Mamba Mamba is a drop-in replacement for Conda. Simply use mamba instead of conda for faster operations: Create a new environment: bash mamba create -n myenv python=3.11 numpy pandas Activate an environment: bash conda activate myenv Install packages: bash mamba install scipy matplotlib Update packages: bash mamba update --all List environments: bash mamba env list Remove an environment: bash mamba env remove -n myenv Tips and Best Practices Always use the conda-forge channel for the latest, community-maintained packages. Why Use conda-forge? Using the conda-forge channel is important because it ensures all packages in your environment are built and maintained by the same community, following consistent standards. This greatly reduces the risk of dependency conflicts and broken environments that can occur when mixing packages from different channels. Conda-forge provides up-to-date versions, broad compatibility, and reliable builds for scientific and data science packages. By sticking to conda-forge, you make your environment more reproducible and robust. To update Mamba itself: bash mamba update mamba For troubleshooting, use verbose mode: bash mamba install <package> --verbose Further Reading Mamba Documentation Miniforge GitHub Conda-forge Mamba makes managing scientific Python environments fast, reliable, and reproducible. Enjoy your new setup!","title":"Mamba Set Up"},{"location":"conda_setup/#guide-to-setting-up-mamba-conda-on-a-linux-machine","text":"Author: Callum","title":"Guide to Setting Up Mamba (Conda) on a Linux Machine"},{"location":"conda_setup/#what-is-mamba","text":"Mamba is a fast, robust, and user-friendly package manager for managing environments and packages in the Conda ecosystem. It is a drop-in replacement for the Conda command-line tool, written in C++ for speed and efficiency. Mamba uses the same environment and package specifications as Conda, but resolves dependencies and installs packages much faster, making it ideal for scientific computing, data science, and reproducible research.","title":"What is Mamba?"},{"location":"conda_setup/#why-use-mamba","text":"Speed : Mamba is significantly faster than Conda, especially for solving complex dependencies and installing large packages. Reliability : Mamba uses parallel downloading and a more efficient dependency resolver, reducing installation errors and timeouts. Compatibility : Mamba works seamlessly with existing Conda environments and packages. You can use mamba and conda commands interchangeably. Community Support : Mamba is developed and maintained by the open-source community, with strong support from the conda-forge project.","title":"Why Use Mamba?"},{"location":"conda_setup/#installing-mamba-via-miniforge","text":"Miniforge is a minimal installer for Conda and Mamba, maintained by conda-forge. It is recommended for Unix-like platforms (Linux, macOS, WSL) and comes with Mamba pre-installed.","title":"Installing Mamba via Miniforge"},{"location":"conda_setup/#step-1-download-the-miniforge-installer","text":"Open a terminal and download the installer appropriate for your system architecture. You can use wget : Using wget: wget \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"","title":"Step 1: Download the Miniforge Installer"},{"location":"conda_setup/#step-2-run-the-installer-script","text":"Run the downloaded script to install Miniforge (and Mamba): bash Miniforge3-$(uname)-$(uname -m).sh Follow the prompts to complete the installation. By default, Miniforge will install to your home directory (e.g., ~/miniforge3 ).","title":"Step 2: Run the Installer Script"},{"location":"conda_setup/#step-3-initialize-conda","text":"After installation, initialize Conda in your shell: source ~/miniforge3/bin/activate conda init Restart your terminal to activate Conda automatically.","title":"Step 3: Initialize Conda"},{"location":"conda_setup/#using-mamba","text":"Mamba is a drop-in replacement for Conda. Simply use mamba instead of conda for faster operations: Create a new environment: bash mamba create -n myenv python=3.11 numpy pandas Activate an environment: bash conda activate myenv Install packages: bash mamba install scipy matplotlib Update packages: bash mamba update --all List environments: bash mamba env list Remove an environment: bash mamba env remove -n myenv","title":"Using Mamba"},{"location":"conda_setup/#tips-and-best-practices","text":"Always use the conda-forge channel for the latest, community-maintained packages.","title":"Tips and Best Practices"},{"location":"conda_setup/#why-use-conda-forge","text":"Using the conda-forge channel is important because it ensures all packages in your environment are built and maintained by the same community, following consistent standards. This greatly reduces the risk of dependency conflicts and broken environments that can occur when mixing packages from different channels. Conda-forge provides up-to-date versions, broad compatibility, and reliable builds for scientific and data science packages. By sticking to conda-forge, you make your environment more reproducible and robust. To update Mamba itself: bash mamba update mamba For troubleshooting, use verbose mode: bash mamba install <package> --verbose","title":"Why Use conda-forge?"},{"location":"conda_setup/#further-reading","text":"Mamba Documentation Miniforge GitHub Conda-forge Mamba makes managing scientific Python environments fast, reliable, and reproducible. Enjoy your new setup!","title":"Further Reading"},{"location":"dask_demo/","text":"Speeding up xarray calculations using dask This notebook demonstrates how to use the dask package to parallelise operations in xarray which (if used correctly and with some luck!) can massively speed up xarray. For this to work you first need to have installed dask (and xarray , netcdf4 , etc.) This demonstration uses some ERA5 data which happens to be in .grib format but this should work with any data once it is opened using xarray First import packages, open the dataset, and define a decorator function that can be used to time functions. Note that you do not need to import dask . But we do import dask.diagnostic 's useful ProgressBar import xarray as xr import pandas as pd import numpy as np from dask.diagnostics import ProgressBar import time ds = xr.open_dataset('/nfs/a68/eebjs/hardknott/drought/vpd_variables.grib', engine='cfgrib') # I got ChatGPT to write me this! def timer(func): def wrapper(*args, **kwargs): start_time = time.time() result = func(*args, **kwargs) end_time = time.time() print(f\"Function {func.__name__}\", f\"took {round(end_time - start_time)}\", \"seconds to execute.\") return result return wrapper On a multi-cored machine, dask will speed up xarray operations by performing calculations on multiple cores at the same time. For this to happen, the dataset/dataarray first has to be 'chunked.' For example, if you have a 1000x1000 sized da with dimensions x and y , you could split it into four 500x500 chunks: da = da.chunk({'x': 2, 'y': 2}) The cpu_count function from the multiprocessing module can check how may cores the machine you are using has: from multiprocessing import cpu_count print(cpu_count()) 32 The ds we have loaded in has three variables, surface pressure ( sp ), 2m temperature ( t2m ) and 2m dew point temperature ( d2m ). It is about 8Gb print(ds) <xarray.Dataset> Dimensions: (time: 702, latitude: 1501, longitude: 3600) Coordinates: number int64 ... * time (time) datetime64[ns] 1965-01-01 1965-02-01 ... 2023-06-01 step timedelta64[ns] ... surface float64 ... * latitude (latitude) float64 75.0 74.9 74.8 74.7 ... -74.8 -74.9 -75.0 * longitude (longitude) float64 -180.0 -179.9 -179.8 ... 179.7 179.8 179.9 valid_time (time) datetime64[ns] ... Data variables: d2m (time, latitude, longitude) float32 ... t2m (time, latitude, longitude) float32 ... sp (time, latitude, longitude) float32 ... Attributes: GRIB_edition: 1 GRIB_centre: ecmf GRIB_centreDescription: European Centre for Medium-Range Weather Forecasts GRIB_subCentre: 0 Conventions: CF-1.7 institution: European Centre for Medium-Range Weather Forecasts history: 2024-03-21T14:46 GRIB to CDM+CF via cfgrib-0.9.1... These will be used to calculate vapour pressure deficit (vpd). This involves performing several operations using all three variables of the dataset. The calculation is performed using the following function. def calculate_vpd(ds): t2m_c = ds['t2m'] - 273.15 d2m_c = ds['d2m'] - 273.15 sp_mb = ds['sp'] / 100 # first calculate saturated vapour pressure fw = 1 + 7e-4 + 3.46e-6 * sp_mb svp = 6.112 * fw * np.exp( (17.67 * t2m_c) / (t2m_c + 243.5) ) # then calculate actual vapour pressure avp = 6.112 * fw * np.exp( (17.67 * d2m_c) / (d2m_c + 243.5) ) # then vapour pressure deficit is: vpd = svp - avp return vpd Without using dask First to get a baseline, we calculate without chunking (i.e. NOT using dask) @timer def calculate_vpd_without_chunking(ds): vpd = calculate_vpd(ds) return vpd vpd = calculate_vpd_without_chunking(ds) Function calculate_vpd_without_chunking took 334 seconds to execute. Using dask When you perform calculations with dask (e.g. ds * 2 ), the result will not yet be returned. Instead a 'delayed' result will be returned. The result can then either be calculated by calling the .compute() method, or it can be using to perform further operation where it will again return a delayed result. In this way, you can 'queue up` multiple operations and then perform them all at the same time. When you call .compute() , an 'unchunked' dataset with the operation performed will be returned. If you want to keep the chunks but still return the result, you can instead call .persist() Now the same thing but using dask: @timer def calculate_vpd_with_chunking(ds): ds = ds.chunk({'time':24}) vpd = calculate_vpd(ds) with ProgressBar(): vpd = vpd.compute(num_workers=30, scheduler='threads') return vpd vpd = calculate_vpd_with_chunking(ds) [########################################] | 100% Completed | 16.68 s Function calculate_vpd_with_chunking took 22 seconds to execute. In this case, the chunked version only took 22 seconds to run, over 15 times quicker than when not using dask. Which dimensions to chunk? In the above example, it shouldn't make too much of a difference which dimensions are chunked, because none of the calculations are being performed across dimensions. However, in the next example, we will take the rolling temporal mean, by calculating the annual means for each 36 months during 1965-2023. In this case, it is faster to calculate across dimensions that are not being reduced in the calculation (i.e. lat and lon). This is because it will be more efficient to calculate the mean if the time dimension is not chunked, so one core has all the values it needs to calculate the mean for a latlon gridcell, rather than time being split across cores. First establish a baseline by performing the calculation without any chunking: @timer def calculate_spatial_mean_vpd_without_chunking(vpd): mean_vpd = vpd.rolling({'time':36}).mean(dim='time') return mean_vpd mean_vpd = calculate_spatial_mean_vpd_without_chunking(ds) Function calculate_spatial_mean_vpd_without_chunking took 283 seconds to execute. Now chunking across time (the WRONG way) @timer def calculate_mean_vpd_chunking_across_time(vpd): # chunk across time vpd = vpd.chunk({'time':24}) # compute the mean with ProgressBar(): mean_vpd = vpd.rolling({'time':36}).mean('time').compute(num_workers=30, scheduler='threads') return mean_vpd mean_vpd = calculate_mean_vpd_chunking_across_time(vpd) [########################################] | 100% Completed | 79.81 s Function calculate_mean_vpd_chunking_across_time took 105 seconds to execute. @timer def calculate_mean_vpd_chunking_across_space(vpd): # chunk across time vpd = vpd.chunk({'latitude':300, 'longitude':600}) # compute the mean with ProgressBar(): mean_vpd = vpd.rolling({'time':36}).mean('time').compute(num_workers=30, scheduler='threads') return mean_vpd mean_vpd = calculate_mean_vpd_chunking_across_space(vpd) [########################################] | 100% Completed | 43.50 ss Function calculate_mean_vpd_chunking_across_space took 67 seconds to execute. In this example chunking the 'right' way was only about 1.5x faster than the wrong way, but this could be much more important depending on the shape of your array and types of operations being performed. In any case chunking was faster (3-4x) than doing the sample calculation without using dask. Note that I tried to keep the total number of chunks similar in both examples to make this a fair comparison. The effect of chunk size Chunk size is another important thing to get right. It is usually worth playing around with a bit to make sure the size is efficient. Usually I try to chunk the array so the total number of chunks is about the same as the number of cores I am using (i.e. the num_workers argument) The below code tests a range of chunk sizes in the time take to calculate the mean of vpd da = vpd[:, :200, :200] # taking a smaller slice of the array so that the calculation is faster def calculate_mean(da, chunksize): # time the chunking overhead start = time.time() da = da.chunk({'time':chunksize}) end = time.time() chunking_time = end-start # time the actual calculation start = time.time() _ = da.mean(['latitude', 'longitude']).compute(num_workers=30, scheduler='threads') end = time.time() calculation_time = end-start return pd.Series( [chunking_time, calculation_time], index=['chunking time', 'calculation time'] ) results = pd.DataFrame() for chunksize in [2,4,8,16,32,64,128, 256, 512]: results[chunksize] = calculate_mean(da, chunksize=chunksize) results.T.plot.bar(grid=True, xlabel='chunksize', ylabel='time (seconds)', stacked=True) In this case the optimum chunksize seems to be around 64. The chunking time does not seem to vary much, but the calculation time does. Let's try the same thing chunking across lat/lon da = vpd[:10] # taking a smaller slice of the array so that the calculation is faster def calculate_mean(da, chunksize): # time the chunking overhead start = time.time() da = da.chunk({'latitude':chunksize, 'longitude':chunksize}) end = time.time() chunking_time = end-start # time the actual calculation start = time.time() _ = da.mean(['time']).compute(num_workers=30, scheduler='threads') end = time.time() calculation_time = end-start return pd.Series( [chunking_time, calculation_time], index=['chunking time', 'calculation time'] ) results = pd.DataFrame() for chunksize in [20,40,80,160,320,640,1280, 2560]: results[chunksize] = calculate_mean(da, chunksize=chunksize) results.T.plot.bar(grid=True, xlabel='chunksize', ylabel='time (seconds)', stacked=True) In this case it seems that larger chunks give a much better performance, with 640 being optimum. You can also chunk by setting the total number of chunks or letting dask choose the number of chunks with chunks='auto' da = vpd[:, :200, :200] # taking a smaller slice of the array so that the calculation is faster def calculate_mean(da, chunksize): # time the chunking overhead start = time.time() da = da.chunk(chunksize) end = time.time() chunking_time = end-start # time the actual calculation start = time.time() _ = da.mean(['latitude', 'longitude']).compute(num_workers=30, scheduler='threads') end = time.time() calculation_time = end-start return pd.Series( [chunking_time, calculation_time], index=['chunking time', 'calculation time'] ) results = pd.DataFrame() for chunksize in ['auto',600, 300,150,75,36,18]: print(chunksize) results[chunksize] = calculate_mean(da, chunksize=chunksize) results.T.plot.bar(grid=True, xlabel='chunksize', ylabel='time (seconds)', stacked=True) But it looks like 'auto' isn't always the best!","title":"Speedy Python with Dask"},{"location":"dask_demo/#speeding-up-xarray-calculations-using-dask","text":"This notebook demonstrates how to use the dask package to parallelise operations in xarray which (if used correctly and with some luck!) can massively speed up xarray. For this to work you first need to have installed dask (and xarray , netcdf4 , etc.) This demonstration uses some ERA5 data which happens to be in .grib format but this should work with any data once it is opened using xarray First import packages, open the dataset, and define a decorator function that can be used to time functions. Note that you do not need to import dask . But we do import dask.diagnostic 's useful ProgressBar import xarray as xr import pandas as pd import numpy as np from dask.diagnostics import ProgressBar import time ds = xr.open_dataset('/nfs/a68/eebjs/hardknott/drought/vpd_variables.grib', engine='cfgrib') # I got ChatGPT to write me this! def timer(func): def wrapper(*args, **kwargs): start_time = time.time() result = func(*args, **kwargs) end_time = time.time() print(f\"Function {func.__name__}\", f\"took {round(end_time - start_time)}\", \"seconds to execute.\") return result return wrapper On a multi-cored machine, dask will speed up xarray operations by performing calculations on multiple cores at the same time. For this to happen, the dataset/dataarray first has to be 'chunked.' For example, if you have a 1000x1000 sized da with dimensions x and y , you could split it into four 500x500 chunks: da = da.chunk({'x': 2, 'y': 2}) The cpu_count function from the multiprocessing module can check how may cores the machine you are using has: from multiprocessing import cpu_count print(cpu_count()) 32 The ds we have loaded in has three variables, surface pressure ( sp ), 2m temperature ( t2m ) and 2m dew point temperature ( d2m ). It is about 8Gb print(ds) <xarray.Dataset> Dimensions: (time: 702, latitude: 1501, longitude: 3600) Coordinates: number int64 ... * time (time) datetime64[ns] 1965-01-01 1965-02-01 ... 2023-06-01 step timedelta64[ns] ... surface float64 ... * latitude (latitude) float64 75.0 74.9 74.8 74.7 ... -74.8 -74.9 -75.0 * longitude (longitude) float64 -180.0 -179.9 -179.8 ... 179.7 179.8 179.9 valid_time (time) datetime64[ns] ... Data variables: d2m (time, latitude, longitude) float32 ... t2m (time, latitude, longitude) float32 ... sp (time, latitude, longitude) float32 ... Attributes: GRIB_edition: 1 GRIB_centre: ecmf GRIB_centreDescription: European Centre for Medium-Range Weather Forecasts GRIB_subCentre: 0 Conventions: CF-1.7 institution: European Centre for Medium-Range Weather Forecasts history: 2024-03-21T14:46 GRIB to CDM+CF via cfgrib-0.9.1... These will be used to calculate vapour pressure deficit (vpd). This involves performing several operations using all three variables of the dataset. The calculation is performed using the following function. def calculate_vpd(ds): t2m_c = ds['t2m'] - 273.15 d2m_c = ds['d2m'] - 273.15 sp_mb = ds['sp'] / 100 # first calculate saturated vapour pressure fw = 1 + 7e-4 + 3.46e-6 * sp_mb svp = 6.112 * fw * np.exp( (17.67 * t2m_c) / (t2m_c + 243.5) ) # then calculate actual vapour pressure avp = 6.112 * fw * np.exp( (17.67 * d2m_c) / (d2m_c + 243.5) ) # then vapour pressure deficit is: vpd = svp - avp return vpd","title":"Speeding up xarray calculations using dask"},{"location":"dask_demo/#without-using-dask","text":"First to get a baseline, we calculate without chunking (i.e. NOT using dask) @timer def calculate_vpd_without_chunking(ds): vpd = calculate_vpd(ds) return vpd vpd = calculate_vpd_without_chunking(ds) Function calculate_vpd_without_chunking took 334 seconds to execute.","title":"Without using dask"},{"location":"dask_demo/#using-dask","text":"When you perform calculations with dask (e.g. ds * 2 ), the result will not yet be returned. Instead a 'delayed' result will be returned. The result can then either be calculated by calling the .compute() method, or it can be using to perform further operation where it will again return a delayed result. In this way, you can 'queue up` multiple operations and then perform them all at the same time. When you call .compute() , an 'unchunked' dataset with the operation performed will be returned. If you want to keep the chunks but still return the result, you can instead call .persist() Now the same thing but using dask: @timer def calculate_vpd_with_chunking(ds): ds = ds.chunk({'time':24}) vpd = calculate_vpd(ds) with ProgressBar(): vpd = vpd.compute(num_workers=30, scheduler='threads') return vpd vpd = calculate_vpd_with_chunking(ds) [########################################] | 100% Completed | 16.68 s Function calculate_vpd_with_chunking took 22 seconds to execute. In this case, the chunked version only took 22 seconds to run, over 15 times quicker than when not using dask.","title":"Using dask"},{"location":"dask_demo/#which-dimensions-to-chunk","text":"In the above example, it shouldn't make too much of a difference which dimensions are chunked, because none of the calculations are being performed across dimensions. However, in the next example, we will take the rolling temporal mean, by calculating the annual means for each 36 months during 1965-2023. In this case, it is faster to calculate across dimensions that are not being reduced in the calculation (i.e. lat and lon). This is because it will be more efficient to calculate the mean if the time dimension is not chunked, so one core has all the values it needs to calculate the mean for a latlon gridcell, rather than time being split across cores. First establish a baseline by performing the calculation without any chunking: @timer def calculate_spatial_mean_vpd_without_chunking(vpd): mean_vpd = vpd.rolling({'time':36}).mean(dim='time') return mean_vpd mean_vpd = calculate_spatial_mean_vpd_without_chunking(ds) Function calculate_spatial_mean_vpd_without_chunking took 283 seconds to execute. Now chunking across time (the WRONG way) @timer def calculate_mean_vpd_chunking_across_time(vpd): # chunk across time vpd = vpd.chunk({'time':24}) # compute the mean with ProgressBar(): mean_vpd = vpd.rolling({'time':36}).mean('time').compute(num_workers=30, scheduler='threads') return mean_vpd mean_vpd = calculate_mean_vpd_chunking_across_time(vpd) [########################################] | 100% Completed | 79.81 s Function calculate_mean_vpd_chunking_across_time took 105 seconds to execute. @timer def calculate_mean_vpd_chunking_across_space(vpd): # chunk across time vpd = vpd.chunk({'latitude':300, 'longitude':600}) # compute the mean with ProgressBar(): mean_vpd = vpd.rolling({'time':36}).mean('time').compute(num_workers=30, scheduler='threads') return mean_vpd mean_vpd = calculate_mean_vpd_chunking_across_space(vpd) [########################################] | 100% Completed | 43.50 ss Function calculate_mean_vpd_chunking_across_space took 67 seconds to execute. In this example chunking the 'right' way was only about 1.5x faster than the wrong way, but this could be much more important depending on the shape of your array and types of operations being performed. In any case chunking was faster (3-4x) than doing the sample calculation without using dask. Note that I tried to keep the total number of chunks similar in both examples to make this a fair comparison.","title":"Which dimensions to chunk?"},{"location":"dask_demo/#the-effect-of-chunk-size","text":"Chunk size is another important thing to get right. It is usually worth playing around with a bit to make sure the size is efficient. Usually I try to chunk the array so the total number of chunks is about the same as the number of cores I am using (i.e. the num_workers argument) The below code tests a range of chunk sizes in the time take to calculate the mean of vpd da = vpd[:, :200, :200] # taking a smaller slice of the array so that the calculation is faster def calculate_mean(da, chunksize): # time the chunking overhead start = time.time() da = da.chunk({'time':chunksize}) end = time.time() chunking_time = end-start # time the actual calculation start = time.time() _ = da.mean(['latitude', 'longitude']).compute(num_workers=30, scheduler='threads') end = time.time() calculation_time = end-start return pd.Series( [chunking_time, calculation_time], index=['chunking time', 'calculation time'] ) results = pd.DataFrame() for chunksize in [2,4,8,16,32,64,128, 256, 512]: results[chunksize] = calculate_mean(da, chunksize=chunksize) results.T.plot.bar(grid=True, xlabel='chunksize', ylabel='time (seconds)', stacked=True) In this case the optimum chunksize seems to be around 64. The chunking time does not seem to vary much, but the calculation time does. Let's try the same thing chunking across lat/lon da = vpd[:10] # taking a smaller slice of the array so that the calculation is faster def calculate_mean(da, chunksize): # time the chunking overhead start = time.time() da = da.chunk({'latitude':chunksize, 'longitude':chunksize}) end = time.time() chunking_time = end-start # time the actual calculation start = time.time() _ = da.mean(['time']).compute(num_workers=30, scheduler='threads') end = time.time() calculation_time = end-start return pd.Series( [chunking_time, calculation_time], index=['chunking time', 'calculation time'] ) results = pd.DataFrame() for chunksize in [20,40,80,160,320,640,1280, 2560]: results[chunksize] = calculate_mean(da, chunksize=chunksize) results.T.plot.bar(grid=True, xlabel='chunksize', ylabel='time (seconds)', stacked=True) In this case it seems that larger chunks give a much better performance, with 640 being optimum. You can also chunk by setting the total number of chunks or letting dask choose the number of chunks with chunks='auto' da = vpd[:, :200, :200] # taking a smaller slice of the array so that the calculation is faster def calculate_mean(da, chunksize): # time the chunking overhead start = time.time() da = da.chunk(chunksize) end = time.time() chunking_time = end-start # time the actual calculation start = time.time() _ = da.mean(['latitude', 'longitude']).compute(num_workers=30, scheduler='threads') end = time.time() calculation_time = end-start return pd.Series( [chunking_time, calculation_time], index=['chunking time', 'calculation time'] ) results = pd.DataFrame() for chunksize in ['auto',600, 300,150,75,36,18]: print(chunksize) results[chunksize] = calculate_mean(da, chunksize=chunksize) results.T.plot.bar(grid=True, xlabel='chunksize', ylabel='time (seconds)', stacked=True) But it looks like 'auto' isn't always the best!","title":"The effect of chunk size"},{"location":"how_to_regrid_xesmf_tutorial/","text":"Tutorial: Regridding 2D NetCDF Datasets in Python with xESMF Author: Callum Regridding (also called remapping or resampling) is a common task in geosciences, especially when working with gridded data such as satellite or climate model outputs. The goal is to interpolate data from one grid to another, which is essential for comparing datasets, combining products, or preparing data for models. In this tutorial, we'll use the Python package xESMF to regrid 2D NetCDF datasets. xESMF is built on top of xarray and ESMF, providing a simple interface for regridding with various algorithms. Prerequisites Install the required packages: mamba install xarray xesmf 1. Loading a NetCDF Dataset We'll use xarray to open NetCDF files. Here, we assume you have a 2D variable (e.g., satellite data) with latitude and longitude coordinates. import xarray as xr ds = xr.open_dataset('input_data.nc') print(ds) Plotting the Input Data import matplotlib.pyplot as plt ds['your_variable'].plot() plt.title('Original Data on Source Grid') plt.show() 2. Defining the Target Grid You need to define the grid you want to regrid to. This can be another dataset's grid, or you can create a new one. Here, we create a regular lat/lon grid: import numpy as np target_grid = xr.Dataset({ 'lat': (['lat'], np.arange(-90, 90.1, 1.0)), 'lon': (['lon'], np.arange(0, 360, 1.0)), }) Visualizing the Target Grid plt.figure() plt.scatter(target_grid['lon'], target_grid['lat'], s=1) plt.xlabel('Longitude') plt.ylabel('Latitude') plt.title('Target Grid Points') plt.show() 3. Regridding with xESMF xESMF supports several regridding methods: Choosing a Regridding Algorithm The choice of algorithm depends on your data and scientific goals: Bilinear : Uses weighted averages of the four nearest grid points. It is smooth and works well for continuous variables (e.g., temperature, pressure). However, it does not conserve the total sum of the variable, so it is not suitable for fluxes or quantities where conservation is important. Conservative : Ensures that the integral (sum) of the variable is preserved during regridding. This is essential for variables like precipitation, runoff, or any fluxes. It requires both source and target grids to define cell boundaries (i.e., grid must be defined by cell centers and edges). It can be sensitive to missing values (NaNs), which may cause the output to be NaN if any input cell is NaN. Conservative-normed : Similar to conservative, but normalizes the weights so that if some source cells are NaN, the valid part of the cell is still used. This is especially useful for satellite data or observational products with missing values, as it avoids propagating NaNs unnecessarily. Use this when you want conservation but need to handle missing data robustly. Nearest_s2d / nearest_d2s : Assigns the value of the nearest source (or destination) grid cell. This is fast and preserves original values, but can introduce blocky artifacts. Use for categorical data (e.g., land/sea masks, land cover types) or when you want to avoid interpolation. Patch : A higher-order method that can provide smoother results for some variables, but is less commonly used and more computationally intensive. Summary Table Algorithm Preserves Integrals Handles NaNs Well Smooth Use For bilinear No Moderate Yes Continuous fields conservative Yes No No Fluxes, precipitation conservative-normed Yes Yes No Fluxes with missing data nearest_s2d/d2s No Yes No Categorical, masks patch No Moderate Yes Advanced, smooth fields Example: Bilinear Regridding import xesmf as xe regridder = xe.Regridder(ds, target_grid, 'bilinear') regridded = regridder(ds['your_variable']) regridded.to_netcdf('output_bilinear.nc') Example: Conservative Regridding regridder_cons = xe.Regridder(ds, target_grid, 'conservative') regridded_cons = regridder_cons(ds['your_variable']) regridded_cons.to_netcdf('output_conservative.nc') Example: Conservative-normed Regridding The conservative-normed method is designed to handle missing values (NaNs) more robustly than standard conservative regridding. In the standard conservative method, if any part of a source cell is NaN, the entire destination cell may become NaN. The conservative-normed method normalizes the weights so that only the valid (non-NaN) fraction of the source cell contributes to the destination cell, preventing unnecessary propagation of NaNs. Using a 'mask' Layer To take full advantage of conservative-normed, you should provide a mask variable in your xarray dataset. This mask should be a DataArray with the same shape as your data, where valid data points are marked as 1 (or True) and missing/invalid points as 0 (or False). xESMF will use this mask to determine which parts of the grid are valid during regridding. Example of adding a mask: import numpy as np # Suppose ds['your_variable'] contains NaNs for missing data mask = (~np.isnan(ds['your_variable'])).astype(int) ds['mask'] = (ds['your_variable'].dims, mask) # Now use conservative-normed regridder_normed = xe.Regridder(ds, target_grid, 'conservative_normed') regridded_normed = regridder_normed(ds['your_variable']) regridded_normed.to_netcdf('output_conservative_normed.nc') If you do not provide a mask, xESMF will infer it from the NaN pattern in your data, but explicitly providing a mask is more robust and recommended for complex or irregular missing data patterns. For more, see the xESMF documentation on masking . Example: Nearest Neighbor Regridding regridder_nn = xe.Regridder(ds, target_grid, 'nearest_s2d') regridded_nn = regridder_nn(ds['your_variable']) regridded_nn.to_netcdf('output_nearest.nc') 4. Considerations Working with Large Datasets Regridding large datasets (e.g., high-resolution satellite data or long time series) can be memory- and compute-intensive. Here are some tips to improve performance: Use Dask for Chunking : xarray and xESMF support dask arrays, which allow you to process data in chunks and parallelize operations. Open your dataset with chunking: ```python ds = xr.open_dataset('input_data.nc', chunks={'time': 10, 'lat': 100, 'lon': 100}) Adjust chunk sizes to fit your memory and data shape ``` Saving and Reusing Regridding Weights : When you create a regridder in xESMF, it computes a weight matrix that maps the source grid to the target grid. This computation can be slow for large grids, but you can save the weights to a file and reload them later for faster repeated regridding. Example: ```python First time: compute and save weights regridder = xe.Regridder(ds, target_grid, 'bilinear', filename='my_weights.nc') Next time: reuse the saved weights (much faster) regridder = xe.Regridder(ds, target_grid, 'bilinear', filename='my_weights.nc', reuse_weights=True) ``` This is especially useful when you need to regrid many variables or process data in chunks, as you only need to compute the weights once. Parallel Processing : If you have access to a cluster or multicore machine, dask can distribute the computation. Set up a dask cluster for even faster processing. Reduce Data Size : If possible, subset your data in time or space before regridding, or use coarser grids for exploratory analysis. Monitor Memory Usage : Large regridding operations can use a lot of RAM. Monitor your system and adjust chunk sizes or process data in smaller batches if needed. For more details, see the xESMF documentation on dask and performance . 5. Visualizing the Results import matplotlib.pyplot as plt regridded.plot() plt.title('Regridded Data (Bilinear)') plt.show() Comparing Input and Output You can compare the original and regridded data side by side: fig, axs = plt.subplots(1, 2, figsize=(12, 5)) ds['your_variable'].plot(ax=axs[0]) axs[0].set_title('Original Data') regridded.plot(ax=axs[1]) axs[1].set_title('Regridded Data (Bilinear)') plt.tight_layout() plt.show() For more advanced analysis, you can plot the difference: diff = regridded - ds['your_variable'].interp_like(regridded) diff.plot() plt.title('Difference After Regridding') plt.show() 6. References xESMF Documentation xarray Documentation This tutorial should help you get started with regridding 2D NetCDF datasets in Python using xESMF. Adjust the code to your specific data and needs!","title":"Regridding with Python"},{"location":"how_to_regrid_xesmf_tutorial/#tutorial-regridding-2d-netcdf-datasets-in-python-with-xesmf","text":"Author: Callum Regridding (also called remapping or resampling) is a common task in geosciences, especially when working with gridded data such as satellite or climate model outputs. The goal is to interpolate data from one grid to another, which is essential for comparing datasets, combining products, or preparing data for models. In this tutorial, we'll use the Python package xESMF to regrid 2D NetCDF datasets. xESMF is built on top of xarray and ESMF, providing a simple interface for regridding with various algorithms.","title":"Tutorial: Regridding 2D NetCDF Datasets in Python with xESMF"},{"location":"how_to_regrid_xesmf_tutorial/#prerequisites","text":"Install the required packages: mamba install xarray xesmf","title":"Prerequisites"},{"location":"how_to_regrid_xesmf_tutorial/#1-loading-a-netcdf-dataset","text":"We'll use xarray to open NetCDF files. Here, we assume you have a 2D variable (e.g., satellite data) with latitude and longitude coordinates. import xarray as xr ds = xr.open_dataset('input_data.nc') print(ds)","title":"1. Loading a NetCDF Dataset"},{"location":"how_to_regrid_xesmf_tutorial/#plotting-the-input-data","text":"import matplotlib.pyplot as plt ds['your_variable'].plot() plt.title('Original Data on Source Grid') plt.show()","title":"Plotting the Input Data"},{"location":"how_to_regrid_xesmf_tutorial/#2-defining-the-target-grid","text":"You need to define the grid you want to regrid to. This can be another dataset's grid, or you can create a new one. Here, we create a regular lat/lon grid: import numpy as np target_grid = xr.Dataset({ 'lat': (['lat'], np.arange(-90, 90.1, 1.0)), 'lon': (['lon'], np.arange(0, 360, 1.0)), })","title":"2. Defining the Target Grid"},{"location":"how_to_regrid_xesmf_tutorial/#visualizing-the-target-grid","text":"plt.figure() plt.scatter(target_grid['lon'], target_grid['lat'], s=1) plt.xlabel('Longitude') plt.ylabel('Latitude') plt.title('Target Grid Points') plt.show()","title":"Visualizing the Target Grid"},{"location":"how_to_regrid_xesmf_tutorial/#3-regridding-with-xesmf","text":"xESMF supports several regridding methods:","title":"3. Regridding with xESMF"},{"location":"how_to_regrid_xesmf_tutorial/#choosing-a-regridding-algorithm","text":"The choice of algorithm depends on your data and scientific goals: Bilinear : Uses weighted averages of the four nearest grid points. It is smooth and works well for continuous variables (e.g., temperature, pressure). However, it does not conserve the total sum of the variable, so it is not suitable for fluxes or quantities where conservation is important. Conservative : Ensures that the integral (sum) of the variable is preserved during regridding. This is essential for variables like precipitation, runoff, or any fluxes. It requires both source and target grids to define cell boundaries (i.e., grid must be defined by cell centers and edges). It can be sensitive to missing values (NaNs), which may cause the output to be NaN if any input cell is NaN. Conservative-normed : Similar to conservative, but normalizes the weights so that if some source cells are NaN, the valid part of the cell is still used. This is especially useful for satellite data or observational products with missing values, as it avoids propagating NaNs unnecessarily. Use this when you want conservation but need to handle missing data robustly. Nearest_s2d / nearest_d2s : Assigns the value of the nearest source (or destination) grid cell. This is fast and preserves original values, but can introduce blocky artifacts. Use for categorical data (e.g., land/sea masks, land cover types) or when you want to avoid interpolation. Patch : A higher-order method that can provide smoother results for some variables, but is less commonly used and more computationally intensive.","title":"Choosing a Regridding Algorithm"},{"location":"how_to_regrid_xesmf_tutorial/#summary-table","text":"Algorithm Preserves Integrals Handles NaNs Well Smooth Use For bilinear No Moderate Yes Continuous fields conservative Yes No No Fluxes, precipitation conservative-normed Yes Yes No Fluxes with missing data nearest_s2d/d2s No Yes No Categorical, masks patch No Moderate Yes Advanced, smooth fields","title":"Summary Table"},{"location":"how_to_regrid_xesmf_tutorial/#example-bilinear-regridding","text":"import xesmf as xe regridder = xe.Regridder(ds, target_grid, 'bilinear') regridded = regridder(ds['your_variable']) regridded.to_netcdf('output_bilinear.nc')","title":"Example: Bilinear Regridding"},{"location":"how_to_regrid_xesmf_tutorial/#example-conservative-regridding","text":"regridder_cons = xe.Regridder(ds, target_grid, 'conservative') regridded_cons = regridder_cons(ds['your_variable']) regridded_cons.to_netcdf('output_conservative.nc')","title":"Example: Conservative Regridding"},{"location":"how_to_regrid_xesmf_tutorial/#example-conservative-normed-regridding","text":"The conservative-normed method is designed to handle missing values (NaNs) more robustly than standard conservative regridding. In the standard conservative method, if any part of a source cell is NaN, the entire destination cell may become NaN. The conservative-normed method normalizes the weights so that only the valid (non-NaN) fraction of the source cell contributes to the destination cell, preventing unnecessary propagation of NaNs.","title":"Example: Conservative-normed Regridding"},{"location":"how_to_regrid_xesmf_tutorial/#using-a-mask-layer","text":"To take full advantage of conservative-normed, you should provide a mask variable in your xarray dataset. This mask should be a DataArray with the same shape as your data, where valid data points are marked as 1 (or True) and missing/invalid points as 0 (or False). xESMF will use this mask to determine which parts of the grid are valid during regridding. Example of adding a mask: import numpy as np # Suppose ds['your_variable'] contains NaNs for missing data mask = (~np.isnan(ds['your_variable'])).astype(int) ds['mask'] = (ds['your_variable'].dims, mask) # Now use conservative-normed regridder_normed = xe.Regridder(ds, target_grid, 'conservative_normed') regridded_normed = regridder_normed(ds['your_variable']) regridded_normed.to_netcdf('output_conservative_normed.nc') If you do not provide a mask, xESMF will infer it from the NaN pattern in your data, but explicitly providing a mask is more robust and recommended for complex or irregular missing data patterns. For more, see the xESMF documentation on masking .","title":"Using a 'mask' Layer"},{"location":"how_to_regrid_xesmf_tutorial/#example-nearest-neighbor-regridding","text":"regridder_nn = xe.Regridder(ds, target_grid, 'nearest_s2d') regridded_nn = regridder_nn(ds['your_variable']) regridded_nn.to_netcdf('output_nearest.nc')","title":"Example: Nearest Neighbor Regridding"},{"location":"how_to_regrid_xesmf_tutorial/#4-considerations","text":"","title":"4. Considerations"},{"location":"how_to_regrid_xesmf_tutorial/#working-with-large-datasets","text":"Regridding large datasets (e.g., high-resolution satellite data or long time series) can be memory- and compute-intensive. Here are some tips to improve performance: Use Dask for Chunking : xarray and xESMF support dask arrays, which allow you to process data in chunks and parallelize operations. Open your dataset with chunking: ```python ds = xr.open_dataset('input_data.nc', chunks={'time': 10, 'lat': 100, 'lon': 100})","title":"Working with Large Datasets"},{"location":"how_to_regrid_xesmf_tutorial/#adjust-chunk-sizes-to-fit-your-memory-and-data-shape","text":"``` Saving and Reusing Regridding Weights : When you create a regridder in xESMF, it computes a weight matrix that maps the source grid to the target grid. This computation can be slow for large grids, but you can save the weights to a file and reload them later for faster repeated regridding. Example: ```python","title":"Adjust chunk sizes to fit your memory and data shape"},{"location":"how_to_regrid_xesmf_tutorial/#first-time-compute-and-save-weights","text":"regridder = xe.Regridder(ds, target_grid, 'bilinear', filename='my_weights.nc')","title":"First time: compute and save weights"},{"location":"how_to_regrid_xesmf_tutorial/#next-time-reuse-the-saved-weights-much-faster","text":"regridder = xe.Regridder(ds, target_grid, 'bilinear', filename='my_weights.nc', reuse_weights=True) ``` This is especially useful when you need to regrid many variables or process data in chunks, as you only need to compute the weights once. Parallel Processing : If you have access to a cluster or multicore machine, dask can distribute the computation. Set up a dask cluster for even faster processing. Reduce Data Size : If possible, subset your data in time or space before regridding, or use coarser grids for exploratory analysis. Monitor Memory Usage : Large regridding operations can use a lot of RAM. Monitor your system and adjust chunk sizes or process data in smaller batches if needed. For more details, see the xESMF documentation on dask and performance .","title":"Next time: reuse the saved weights (much faster)"},{"location":"how_to_regrid_xesmf_tutorial/#5-visualizing-the-results","text":"import matplotlib.pyplot as plt regridded.plot() plt.title('Regridded Data (Bilinear)') plt.show()","title":"5. Visualizing the Results"},{"location":"how_to_regrid_xesmf_tutorial/#comparing-input-and-output","text":"You can compare the original and regridded data side by side: fig, axs = plt.subplots(1, 2, figsize=(12, 5)) ds['your_variable'].plot(ax=axs[0]) axs[0].set_title('Original Data') regridded.plot(ax=axs[1]) axs[1].set_title('Regridded Data (Bilinear)') plt.tight_layout() plt.show() For more advanced analysis, you can plot the difference: diff = regridded - ds['your_variable'].interp_like(regridded) diff.plot() plt.title('Difference After Regridding') plt.show()","title":"Comparing Input and Output"},{"location":"how_to_regrid_xesmf_tutorial/#6-references","text":"xESMF Documentation xarray Documentation This tutorial should help you get started with regridding 2D NetCDF datasets in Python using xESMF. Adjust the code to your specific data and needs!","title":"6. References"},{"location":"linux_cheat_sheet/","text":"Linux Command Line Cheat Sheet Author: Callum Introduction The Linux command line is a powerful tool for interacting with your computer. It allows you to navigate files, manage processes, and perform a wide range of tasks efficiently. Below are some essential commands and tips to get you started. How to Access the Command Line You can access the Linux command line in several ways, depending on your operating system and preferences: Terminal (Linux/macOS): Most Linux distributions and macOS have a built-in Terminal app. MobaXterm (Windows): A popular terminal emulator for Windows that provides SSH, SFTP, and more for connecting to Linux systems. Windows Subsystem for Linux (WSL): Allows you to run a Linux environment directly on Windows. Open via the Windows Terminal or the WSL app. VS Code Terminal: Visual Studio Code has an integrated terminal that can be used for command line access on any OS. PuTTY (Windows): Lightweight SSH client for connecting to remote Linux servers. Jupyter Notebooks: Some notebooks provide a terminal tab for command line access. Choose the method that best fits your workflow and system setup. Basic Navigation Command Description pwd Print current working directory ls List files and directories ls -l List with details (long format) ls -a List all files, including hidden cd <dir> Change directory to <dir> cd .. Go up one directory level cd ~ Go to your home directory mkdir <dir> Create a new directory rmdir <dir> Remove an empty directory touch <file> Create an empty file rm <file> Remove a file cp <src> <dest> Copy file or directory mv <src> <dest> Move or rename file/directory Viewing and Editing Files Command Description cat <file> Display file contents less <file> View file one page at a time head <file> Show first 10 lines of a file tail <file> Show last 10 lines of a file nano <file> Edit file with nano text editor vim <file> Edit file with vim text editor grep 'text' <file> Search for 'text' in a file System Info & Management Command Description whoami Show current user date Show current date and time df -h Show disk space usage du -sh <dir> Show size of a directory free -h Show memory usage top Show running processes ps aux List all running processes kill <pid> Kill process with process ID File Permissions Command Description chmod +x <file> Make file executable chmod 755 <file> Set permissions to rwxr-xr-x chown user:group <file> Change file owner and group Networking Command Description ping <host> Test network connection to host curl <url> Fetch content from a URL wget <url> Download file from a URL ssh user@host Connect to remote host via SSH Other Useful Commands Command Description history Show command history man <command> Show manual for a command echo \"text\" Print text to terminal tar -xzvf <file.tar.gz> Extract a tar.gz archive zip/unzip <file.zip> Compress or extract zip files Tips Use Tab for auto-completion. Use Ctrl+C to stop a running command. Use Ctrl+R to search command history. Use && to chain commands (run next if previous succeeds). For more, see the GNU Core Utilities Manual or run man <command> in your terminal.","title":"Linux Cheat Sheet"},{"location":"linux_cheat_sheet/#linux-command-line-cheat-sheet","text":"Author: Callum","title":"Linux Command Line Cheat Sheet"},{"location":"linux_cheat_sheet/#introduction","text":"The Linux command line is a powerful tool for interacting with your computer. It allows you to navigate files, manage processes, and perform a wide range of tasks efficiently. Below are some essential commands and tips to get you started.","title":"Introduction"},{"location":"linux_cheat_sheet/#how-to-access-the-command-line","text":"You can access the Linux command line in several ways, depending on your operating system and preferences: Terminal (Linux/macOS): Most Linux distributions and macOS have a built-in Terminal app. MobaXterm (Windows): A popular terminal emulator for Windows that provides SSH, SFTP, and more for connecting to Linux systems. Windows Subsystem for Linux (WSL): Allows you to run a Linux environment directly on Windows. Open via the Windows Terminal or the WSL app. VS Code Terminal: Visual Studio Code has an integrated terminal that can be used for command line access on any OS. PuTTY (Windows): Lightweight SSH client for connecting to remote Linux servers. Jupyter Notebooks: Some notebooks provide a terminal tab for command line access. Choose the method that best fits your workflow and system setup.","title":"How to Access the Command Line"},{"location":"linux_cheat_sheet/#basic-navigation","text":"Command Description pwd Print current working directory ls List files and directories ls -l List with details (long format) ls -a List all files, including hidden cd <dir> Change directory to <dir> cd .. Go up one directory level cd ~ Go to your home directory mkdir <dir> Create a new directory rmdir <dir> Remove an empty directory touch <file> Create an empty file rm <file> Remove a file cp <src> <dest> Copy file or directory mv <src> <dest> Move or rename file/directory","title":"Basic Navigation"},{"location":"linux_cheat_sheet/#viewing-and-editing-files","text":"Command Description cat <file> Display file contents less <file> View file one page at a time head <file> Show first 10 lines of a file tail <file> Show last 10 lines of a file nano <file> Edit file with nano text editor vim <file> Edit file with vim text editor grep 'text' <file> Search for 'text' in a file","title":"Viewing and Editing Files"},{"location":"linux_cheat_sheet/#system-info-management","text":"Command Description whoami Show current user date Show current date and time df -h Show disk space usage du -sh <dir> Show size of a directory free -h Show memory usage top Show running processes ps aux List all running processes kill <pid> Kill process with process ID","title":"System Info &amp; Management"},{"location":"linux_cheat_sheet/#file-permissions","text":"Command Description chmod +x <file> Make file executable chmod 755 <file> Set permissions to rwxr-xr-x chown user:group <file> Change file owner and group","title":"File Permissions"},{"location":"linux_cheat_sheet/#networking","text":"Command Description ping <host> Test network connection to host curl <url> Fetch content from a URL wget <url> Download file from a URL ssh user@host Connect to remote host via SSH","title":"Networking"},{"location":"linux_cheat_sheet/#other-useful-commands","text":"Command Description history Show command history man <command> Show manual for a command echo \"text\" Print text to terminal tar -xzvf <file.tar.gz> Extract a tar.gz archive zip/unzip <file.zip> Compress or extract zip files","title":"Other Useful Commands"},{"location":"linux_cheat_sheet/#tips","text":"Use Tab for auto-completion. Use Ctrl+C to stop a running command. Use Ctrl+R to search command history. Use && to chain commands (run next if previous succeeds). For more, see the GNU Core Utilities Manual or run man <command> in your terminal.","title":"Tips"},{"location":"printing/","text":"\ud83d\udda8\ufe0f Printing an A1 Poster on 4 Sheets of A3 Author: Elle This guide details how to split an A1 poster into four separate A3 PDF sheets for printing on University of Leeds printers. 1. Initial Setup in PowerPoint and PDF Creation Ensure your poster is correctly formatted as an A1 size document in PowerPoint . Go to the print dialogue. Select the PDF-XChange 5.0 for ABBYY printer option as shown in the image. Note: This acts as a virtual printer to convert the document. 2. Configuring Printer Properties to Split the A1 Page (Critical Step) Click on \"Printer Properties\" . Navigate to the \"Paper\" or \"Paper Settings\" tab. Set the Page Size to A1 (e.g., $594.0 \\times 841.0 \\text{ mm}$). In the \"Layout\" section, set the Sheet Size to A3 . Locate the Position boxes (X and Y coordinates) to define the four A3 sections. You must repeat the print/save process for each position. A1 Poster Split Coordinates A3 Page Section Position X (mm) Position Y (mm) Top-Left (Part 1) 0.0 0.0 Top-Right (Part 2) -297.0 0.0 Bottom-Left (Part 3) 0.0 -420.0 Bottom-Right (Part 4) -297.0 -420.0 Action: Print/Save as a separate PDF file (e.g., Poster_Part1.pdf ) after setting each coordinate pair. 3. Printing the A3 PDF Sheets To successfully print A3 on campus: You must be connected to the VPN . Open each of the four saved A3 PDFs (e.g., in a browser like Chrome). Initiate printing. Select the Staff on LeedsPrint print queue as the Destination . Click on \"More settings\" . Set the Paper size to A3 . Note: The user found this method more reliable than using the MyPrint portal. 4. Alternative Printing Options (If Laptop Fails) If printing from your personal laptop fails or the print queue is unavailable, you have other options: Use a wired computer in the library. Print through the Azure Virtual Desktop (AVD). (Refer to University IT services for up-to-date guidance on these options). 5. Assembly Each A3 sheet will print with a border . To create a fully joined-up A1 poster, you will need to cut the borders off and stick the four A3 sheets together with Sellotape.","title":"Printing A1"},{"location":"printing/#printing-an-a1-poster-on-4-sheets-of-a3","text":"Author: Elle This guide details how to split an A1 poster into four separate A3 PDF sheets for printing on University of Leeds printers.","title":"\ud83d\udda8\ufe0f Printing an A1 Poster on 4 Sheets of A3"},{"location":"printing/#1-initial-setup-in-powerpoint-and-pdf-creation","text":"Ensure your poster is correctly formatted as an A1 size document in PowerPoint . Go to the print dialogue. Select the PDF-XChange 5.0 for ABBYY printer option as shown in the image. Note: This acts as a virtual printer to convert the document.","title":"1. Initial Setup in PowerPoint and PDF Creation"},{"location":"printing/#2-configuring-printer-properties-to-split-the-a1-page-critical-step","text":"Click on \"Printer Properties\" . Navigate to the \"Paper\" or \"Paper Settings\" tab. Set the Page Size to A1 (e.g., $594.0 \\times 841.0 \\text{ mm}$). In the \"Layout\" section, set the Sheet Size to A3 . Locate the Position boxes (X and Y coordinates) to define the four A3 sections. You must repeat the print/save process for each position.","title":"2. Configuring Printer Properties to Split the A1 Page (Critical Step)"},{"location":"printing/#a1-poster-split-coordinates","text":"A3 Page Section Position X (mm) Position Y (mm) Top-Left (Part 1) 0.0 0.0 Top-Right (Part 2) -297.0 0.0 Bottom-Left (Part 3) 0.0 -420.0 Bottom-Right (Part 4) -297.0 -420.0 Action: Print/Save as a separate PDF file (e.g., Poster_Part1.pdf ) after setting each coordinate pair.","title":"A1 Poster Split Coordinates"},{"location":"printing/#3-printing-the-a3-pdf-sheets","text":"To successfully print A3 on campus: You must be connected to the VPN . Open each of the four saved A3 PDFs (e.g., in a browser like Chrome). Initiate printing. Select the Staff on LeedsPrint print queue as the Destination . Click on \"More settings\" . Set the Paper size to A3 . Note: The user found this method more reliable than using the MyPrint portal.","title":"3. Printing the A3 PDF Sheets"},{"location":"printing/#4-alternative-printing-options-if-laptop-fails","text":"If printing from your personal laptop fails or the print queue is unavailable, you have other options: Use a wired computer in the library. Print through the Azure Virtual Desktop (AVD). (Refer to University IT services for up-to-date guidance on these options).","title":"4. Alternative Printing Options (If Laptop Fails)"},{"location":"printing/#5-assembly","text":"Each A3 sheet will print with a border . To create a fully joined-up A1 poster, you will need to cut the borders off and stick the four A3 sheets together with Sellotape.","title":"5. Assembly"},{"location":"python_quick_start/","text":"Python Quick Start Guide Author: Callum Introduction Python is a powerful, easy-to-learn programming language widely used in science, data analysis, web development, automation, and more. This guide will help beginners get started with Python, focusing on best practices for installation, environment management, and basic usage. For scientific and data work, using Conda (or Mamba) is highly recommended\u2014see the Conda Setup Guide for details. Opening a Terminal To use Python and Conda/Mamba, you'll need to open a terminal (command line window) on your computer: Windows: If you have Windows Subsystem for Linux (WSL) installed, open the Start menu, search for \"WSL\" or your chosen Linux distribution (e.g., Ubuntu), and click to open a Linux terminal. Alternatively, use MobaXterm : install it, then open MobaXterm and start a local terminal or a WSL session. Mac: Open the Terminal app (find it in Applications > Utilities, or search for \"Terminal\" in Spotlight). Linux: Open your system's Terminal app (often found in the applications menu, or use Ctrl+Alt+T as a shortcut). Once your terminal is open, you can follow the steps below to install Python, set up environments, and run code. Why Use Python? Simple, readable syntax Large ecosystem of libraries for every purpose Excellent for beginners and experts alike Strong community support Step 1: Install Python (Recommended: via Conda/Mamba) While you can install Python directly from python.org, the best way for scientific work is to use Conda or Mamba. This lets you easily manage multiple Python versions and install packages without conflicts. See the Conda Setup Guide for step-by-step instructions. Step 2: Create a Python Environment Environments keep your projects isolated, so packages for one project don\u2019t interfere with another. mamba create -n mypython python=3.11 mamba activate mypython Step 3: Install Packages Use Mamba (or Conda) to install packages from the conda-forge channel. This ensures compatibility and avoids dependency issues. mamba install numpy pandas matplotlib Step 4: Write and Run Python Code You can write Python code in a text editor (like VS Code) and run it from the terminal: python myscript.py Or use interactive notebooks (Jupyter) for data analysis: mamba install jupyterlab jupyter lab Step 5: Learn the Basics Here are some key concepts for beginners: - Variables : Store data (numbers, text, etc.) - Data types : int, float, str, list, dict, etc. - Control flow : if, for, while - Functions : Reusable blocks of code - Modules : Import libraries to extend Python\u2019s capabilities Example: # myscript.py name = \"World\" print(f\"Hello, {name}!\") Best Practices Always use environments for your projects Prefer conda-forge for package installs Keep your Python and packages up to date Use version control (e.g., git) for your code Read error messages\u2014they help you debug! Useful Resources Conda Setup Guide Python Official Documentation Real Python Tutorials Jupyter Project Troubleshooting If you have issues with packages, check you\u2019re using conda-forge and not mixing channels If Python isn\u2019t found, make sure your environment is activated For help, search error messages or ask the community Python is a great language for learning and doing real-world work. With Conda/Mamba and conda-forge, you\u2019ll avoid most installation headaches and be ready to explore!","title":"Python Quick Start"},{"location":"python_quick_start/#python-quick-start-guide","text":"Author: Callum","title":"Python Quick Start Guide"},{"location":"python_quick_start/#introduction","text":"Python is a powerful, easy-to-learn programming language widely used in science, data analysis, web development, automation, and more. This guide will help beginners get started with Python, focusing on best practices for installation, environment management, and basic usage. For scientific and data work, using Conda (or Mamba) is highly recommended\u2014see the Conda Setup Guide for details.","title":"Introduction"},{"location":"python_quick_start/#opening-a-terminal","text":"To use Python and Conda/Mamba, you'll need to open a terminal (command line window) on your computer: Windows: If you have Windows Subsystem for Linux (WSL) installed, open the Start menu, search for \"WSL\" or your chosen Linux distribution (e.g., Ubuntu), and click to open a Linux terminal. Alternatively, use MobaXterm : install it, then open MobaXterm and start a local terminal or a WSL session. Mac: Open the Terminal app (find it in Applications > Utilities, or search for \"Terminal\" in Spotlight). Linux: Open your system's Terminal app (often found in the applications menu, or use Ctrl+Alt+T as a shortcut). Once your terminal is open, you can follow the steps below to install Python, set up environments, and run code.","title":"Opening a Terminal"},{"location":"python_quick_start/#why-use-python","text":"Simple, readable syntax Large ecosystem of libraries for every purpose Excellent for beginners and experts alike Strong community support","title":"Why Use Python?"},{"location":"python_quick_start/#step-1-install-python-recommended-via-condamamba","text":"While you can install Python directly from python.org, the best way for scientific work is to use Conda or Mamba. This lets you easily manage multiple Python versions and install packages without conflicts. See the Conda Setup Guide for step-by-step instructions.","title":"Step 1: Install Python (Recommended: via Conda/Mamba)"},{"location":"python_quick_start/#step-2-create-a-python-environment","text":"Environments keep your projects isolated, so packages for one project don\u2019t interfere with another. mamba create -n mypython python=3.11 mamba activate mypython","title":"Step 2: Create a Python Environment"},{"location":"python_quick_start/#step-3-install-packages","text":"Use Mamba (or Conda) to install packages from the conda-forge channel. This ensures compatibility and avoids dependency issues. mamba install numpy pandas matplotlib","title":"Step 3: Install Packages"},{"location":"python_quick_start/#step-4-write-and-run-python-code","text":"You can write Python code in a text editor (like VS Code) and run it from the terminal: python myscript.py Or use interactive notebooks (Jupyter) for data analysis: mamba install jupyterlab jupyter lab","title":"Step 4: Write and Run Python Code"},{"location":"python_quick_start/#step-5-learn-the-basics","text":"Here are some key concepts for beginners: - Variables : Store data (numbers, text, etc.) - Data types : int, float, str, list, dict, etc. - Control flow : if, for, while - Functions : Reusable blocks of code - Modules : Import libraries to extend Python\u2019s capabilities Example: # myscript.py name = \"World\" print(f\"Hello, {name}!\")","title":"Step 5: Learn the Basics"},{"location":"python_quick_start/#best-practices","text":"Always use environments for your projects Prefer conda-forge for package installs Keep your Python and packages up to date Use version control (e.g., git) for your code Read error messages\u2014they help you debug!","title":"Best Practices"},{"location":"python_quick_start/#useful-resources","text":"Conda Setup Guide Python Official Documentation Real Python Tutorials Jupyter Project","title":"Useful Resources"},{"location":"python_quick_start/#troubleshooting","text":"If you have issues with packages, check you\u2019re using conda-forge and not mixing channels If Python isn\u2019t found, make sure your environment is activated For help, search error messages or ask the community Python is a great language for learning and doing real-world work. With Conda/Mamba and conda-forge, you\u2019ll avoid most installation headaches and be ready to explore!","title":"Troubleshooting"}]}